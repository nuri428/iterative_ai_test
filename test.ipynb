{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuri/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 9058.97it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1777.25it/s]\n",
      "Generating train split: 5000 examples [00:00, 70186.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files= \"./data/sentiment_train_data/datasets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': '캠핑 다니며 그리들 사용해보니 모든게 맛있고 너무 좋더군요.  그래서 이번에 그리들팬 새로 장만해봤습니다^^  기존에 33cm 쓰다가 바꿀 때 되어서 구입했어요. 36, 40중에서 고민하다 40 샀는데 생각보다많이 크네요. 아무래도 캠핑에 가지고 다니기엔 부피가 너무 크네요. 그래도 가격도 좋고 깊이감도 있어 좋아요. 캠핑은 아직 못 가서 집에서 라면도 먹고 볶음밥도 먹고 고기도 먹었는데 맛있게 잘돼요ㅎ 구성품에 가방까지 있어서 좋아요.  상자가 다 찢어지고 배송은 완전 엉망이었어요.ㅠㅠ 다행히 상품은 이상 없었지만, 처음 찢어진 상자를 봤을땐 많이 놀랬네요 ㅋㅋ  여기까지 만족스러운 그리들 구매 후기를 남겨봤습니다~',\n",
       " 'id': 1035734,\n",
       " 'label': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>주변사람 추천템으로 인기가 많은 상품을 지금부터 소개합니다.  늘 사용하고 있는 제품입니다. 변기에 있는 물때를 깨끗하게 제거해주니 정말 좋은 것 같아요. 또한 오염방지 기능도 있어서 참 좋습니다. 방향제 효과도 있어서 사용할 때 향이 은은하게 나니 더 좋은 것 같아요. 다른 향보다 레몬향이 은은하게 오래 지속되는 점이 참 좋은 것 같네요. 가격이 그렇게 저렴한 편은 아니지만 다 쓰고 나면 재구매할 의사가 있습니다. 너무 좋아요.  내 삶을 더욱 업그레이드해주는 잇템이 될것 같습니다.</td>\n",
       "      <td>1035953</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아기때부터 지금 어른이 된 이 순간까지 꾸준히 제 곁을 지켜준 이가 있는데요. 궁금하시다면 지금부터 후기속으로 빠져보겠습니다.  향이 은은하게 퍼져서 마음까지 편안하게 만들어주는 것 같습니다. 세안뿐만 아니라 바디용으로도 사용하기 좋은 것 같습니다. 세안을 했을 때 당기지 않고 촉촉해서 참 만족스러워요. 유통기한도 2년이나 남아서 엄청 넉넉합니다. 잘 구매한 것 같네요.  오늘은 여기까지 입니다 다소 부족하지만 이해해 주세요 다음에는 다른 내용으로 다시올게요!</td>\n",
       "      <td>1037003</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40대가 되었음에도 ㅇㅇ는 영원한 사랑이네요 오늘은 ㅇㅇ 휴지를 소개해 볼까 합니다  ㅇㅇ를 너무 좋아해서 한 번 주문해봤다가 먼지도 안 나고 부드러워서 몇 년째 사용중입니다일단 디자인이 ㅇㅇ라 사용하면서도 행복하네요 롤도 단단히 감겨 있고 먼지도 거의 안나서 너무 좋습니다 재질도 고급지고 부드러움은 또 말할 수 없이 부드러워요 두께도 사용하기 적당합니다 너무 두꺼워도 불편할 때가 있거든요 가격도 ㅇㅇ지만 저렴하고 행사하면 더 좋은 가격에 구매할 수 있어요 강추합니다  포스팅을 보시고 더 좋은 선택을 할수 있는 의미있는 글이였으면 좋겠네요</td>\n",
       "      <td>1033869</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>오래된 개인주택이라 여기저기 생기는 곰팡이 때문에 고민이 많았는데 지인이 소개해 준 곰팡이 제거제가 있어 소개해드리려구요.  벽지에 결로땜에 곰팡이가 생겨서 뜯어내보니 안쪽에 곰팡이가 가득했어요. 너무 여러군데 곰팡이가 많아서 뿌리고 15분 정도 둔거 같아요. 그리고 젖은 걸레로 닦아냈는데 어머나~!!! 깜쪽같이... 진짜 깨끗하게 닦였어요. 대신 그만큼 냄새도 독한거 같아서 환기를 오랫동안 했지요. 사용할 때 마스크도 잘끼고 했구요...근데 뿌려만 두고 물로 씻어낸다고 해서 깨끗이 닦이지는 않아요. 솔 등으로 문질러주면 효과가 더 좋습니다.  곰팡이 제거 뿐만아니라 효과도 오래 지속되는 제품이라 좋은 정보 드리고 싶었어요. 다음에 또 다른 제품 포스팅 올릴께요.</td>\n",
       "      <td>1038029</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여러분은 지금 어떤 생각을 하고 있습니까? 혹시 ㅇㅇㅇ 치약의 후기를 기다리고 계시진 않았나요? 지금 시작하겠습니다.  잇몸때문에 치과를 다니다가 추천을 받아서 구매를 했습니다. 이시림이 효과가 있을까 생각을 했는데 확실히 좋아졌네요. 이를 닦고 나면 입안이 상쾌해서 좋습니다. 개운함이 오래유지되는 점도 좋은 것 같고요. 가격이 비싼편인데 온라인으로 구매해도 비싼 것 같습니다. 만족스러운 치약을 만난 것 같습니다.  저는 이만 물러갈께요! 부족한 글이지만 끝까지 읽어주셔서 너무 고마워요~ 다음에도 많이 준비 할테니 또 오세요</td>\n",
       "      <td>1036956</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>말복만 지나면 이제 코스모스 피고 아침저녁으로 서늘한 바람이 불 것입니다. 그 생각으로 기운을 차려 오늘을 보내야 하겠습니다.  전에 사용하던 먼지털이개가 너무 오래되고 잘 안 닦여서 새상품으로 구입을 했습니다 화면 그대로 디자인이 예뻐서 맘에 듭니다 차량의 먼지도 시원하게 잘 털려서 아주 좋습니다 극세사라서 차량에 흠집이 안나서 마음에 들어요 손잡이 길이도 늘려져서 사용하기 좋습니다 가격도 저렴해서 딱 좋네요  오늘 내용이 좋은 정보가 되였나요? 그랬다면 좋아요와 댓글 부탁드려요 다음에도 더 좋은 내용으로 찾아뵐께요</td>\n",
       "      <td>1034626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>며칠전 길을 걷다 앞에 걸어가는 사람에게서 나는 은은한 향이 꼭 이 향 같았어요   향도 은은하게 오래 남아서 매일 쓰기 좋아요 가끔 향이 강한 타사 제품 섬유유연제는 머리 가 아픈데 그렇지 않아 좋네요 고급스러운 은은한향이라 맘에 들고 시트형은 처음 사용해봤 는데 정말 편하고 한장씩 낱개포장이라 더 사용하기 편하고 좋아요. 반신반의하며 샀는데 비누향이 정말 좋네요 단점은 좀 비싸네요 여기에서 좋은 가격에 사긴했어요 비싼만큼 효과가 좋으니 만족합니다  늘 사용하는 섬유유연제지만 마음에 드는 제품 없으시다면 이 제품 한번 사용해보시라고 하 고 싶네요</td>\n",
       "      <td>1026944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>안녕하세요 이웃님들~~OOO 바르는 곰팡이싹 사용후기 지금 시작합니다.  욕실 벽 모퉁이에 곰팡이가 생겨 쭉 짜두고 한시간 방치하니 노랗게 곰팡이 부분이 분해되더니 젤도 노란 색으로 바뀌더라구요. 독한 냄새가 납니다. 호흡기엔 좋지 않으니 마스크 쓰고 하는 게 좋다고 봐요. 노랗게 변한 젤을 닦고 환기를 시켰어요. 확연히 없어진 곰팡이를 보니 속이 다 시원합니다. 묽지 않아 주르륵 흘러내리지 않아서 이렇게 애매한 스팟에도 사용 가능하니 좋아요. 정말 고난도의 스킬을 필요로 하지 않아 강추합니다.단, 오래동안 사용하지 않으면 좀 굳어요.  설명 끝!! 좀 도움이 되셨나요? 도움이 되셨다면 전 너무 행복해요~~</td>\n",
       "      <td>1030262</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>많은 분들에게 인기가 많은 ㅇㅇㅇ 화장지 후기를 지금부터 작성하려고 합니다  1+1 행사라 너무 좋네요 ㅇㅇㅇ 비싼 제품이라 늘 망설이는데 이번에 하나 가격으로 보니 너무 저렴한거 같아 바로 구매했습니다 3겹이라 두툼하고 타브랜드 3겹보다 더 두툼합니다 길이도 30m나 되니 좋고 향도 은은해서 더욱 마음에 듭니다 ㅇㅇㅇ 휴지는 먼지도 덜하네요 아예 없는건 아닌데 신경 쓰일 정도는 아닙니다 30롤 2팩해서 60롤이나 되니 오랫동안 잘 사용하겠어요 만족합니다  제 후기로 인해 구매시 많은 도움이 되었으면 좋겠습니다</td>\n",
       "      <td>1033795</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>오늘도 방문해 주셔서 감사합니다. 오늘의 후기는 밀폐용기입니다.  우리 밀폐용기도 이왕 예쁜 게 좋잖아요? 저는 디자인이 이쁘면 저도 모르게 사는 경향이 있 는데요. 이 제품 뚜껑 색이 파스텔 톤으로 너무 예뻐서 색은 정말 마음에 들었는데요.  뚜껑 소재가 좀 저렴한 플라스틱 느낌이어서 오래 쓸 수 있을지는 의문이었어요. 그리고 전자레인지에 사용 가능하다고 해서 돌렸는데 찌그러졌어요. 전자레인지에 오래 돌릴 수 있는 소재는 아닌 것 같아요.  올 한해 잘 마무리하시고 항상 건강하고 행복하세요.</td>\n",
       "      <td>1032748</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 28.8k/28.8k [00:00<00:00, 49.9MB/s]\n",
      "Downloading metadata: 100%|██████████| 28.7k/28.7k [00:00<00:00, 49.7MB/s]\n",
      "Downloading readme: 100%|██████████| 27.9k/27.9k [00:00<00:00, 52.7MB/s]\n",
      "Downloading data: 100%|██████████| 377k/377k [00:00<00:00, 11.5MB/s]\n",
      "Generating train split: 100%|██████████| 8551/8551 [00:00<00:00, 80758.31 examples/s]\n",
      "Generating validation split: 100%|██████████| 1043/1043 [00:00<00:00, 84648.98 examples/s]\n",
      "Generating test split: 100%|██████████| 1063/1063 [00:00<00:00, 79723.65 examples/s]\n",
      "/tmp/ipykernel_42102/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n",
      "Downloading builder script: 5.76kB [00:00, 14.2MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<00:00, 167kB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 483/483 [00:00<00:00, 3.66MB/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 12.2MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 2.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
     ]
    }
   ],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8551/8551 [00:00<00:00, 96076.09 examples/s]\n",
      "Map: 100%|██████████| 1043/1043 [00:00<00:00, 86098.39 examples/s]\n",
      "Map: 100%|██████████| 1063/1063 [00:00<00:00, 71780.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|██████████| 268M/268M [00:23<00:00, 11.4MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb 셀 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m validation_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_mismatched\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmnli-mm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_matched\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmnli\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     args,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mencoded_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mencoded_dataset[validation_key],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/transformers/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_hf_repo()\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m    561\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moutput_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/transformers/trainer.py:3435\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3433\u001b[0m     repo_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id\n\u001b[0;32m-> 3435\u001b[0m repo_url \u001b[39m=\u001b[39m create_repo(repo_name, token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_token, private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m repo_url\u001b[39m.\u001b[39mrepo_id\n\u001b[1;32m   3437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_in_progress \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2542\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_lfsmultipartthresh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2539\u001b[0m     \u001b[39m# Testing purposes only.\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m     \u001b[39m# See https://github.com/huggingface/huggingface_hub/pull/733/files#r820604472\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mlfsmultipartthresh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lfsmultipartthresh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2542\u001b[0m headers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_hf_headers(token\u001b[39m=\u001b[39;49mtoken, is_write_action\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2543\u001b[0m r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mpost(path, headers\u001b[39m=\u001b[39mheaders, json\u001b[39m=\u001b[39mjson)\n\u001b[1;32m   2545\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:5719\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   5716\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5717\u001b[0m     \u001b[39m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   5718\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken\n\u001b[0;32m-> 5719\u001b[0m \u001b[39mreturn\u001b[39;00m build_hf_headers(\n\u001b[1;32m   5720\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   5721\u001b[0m     is_write_action\u001b[39m=\u001b[39;49mis_write_action,\n\u001b[1;32m   5722\u001b[0m     library_name\u001b[39m=\u001b[39;49mlibrary_name \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary_name,\n\u001b[1;32m   5723\u001b[0m     library_version\u001b[39m=\u001b[39;49mlibrary_version \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary_version,\n\u001b[1;32m   5724\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m   5725\u001b[0m )\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:122\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# Get auth token to send\u001b[39;00m\n\u001b[1;32m    121\u001b[0m token_to_send \u001b[39m=\u001b[39m get_token_to_send(token)\n\u001b[0;32m--> 122\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[39m=\u001b[39;49mis_write_action)\n\u001b[1;32m    124\u001b[0m \u001b[39m# Combine headers\u001b[39;00m\n\u001b[1;32m    125\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m    126\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m: _http_user_agent(\n\u001b[1;32m    127\u001b[0m         library_name\u001b[39m=\u001b[39mlibrary_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m }\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:172\u001b[0m, in \u001b[0;36m_validate_token_to_send\u001b[0;34m(token, is_write_action)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m is_write_action:\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mToken is required (write-access action) but no token found. You need\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m https://huggingface.co/settings/tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         )\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mapi_org\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    179\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must use your personal account token for write-access methods. To\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m generate a write-access token, go to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m https://huggingface.co/settings/tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

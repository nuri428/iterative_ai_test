{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nuri/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 9058.97it/s]\n",
      "Extracting data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 1777.25it/s]\n",
      "Generating train split: 5000 examples [00:00, 70186.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files= \"./data/sentiment_train_data/datasets.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'ìº í•‘ ë‹¤ë‹ˆë©° ê·¸ë¦¬ë“¤ ì‚¬ìš©í•´ë³´ë‹ˆ ëª¨ë“ ê²Œ ë§›ìˆê³  ë„ˆë¬´ ì¢‹ë”êµ°ìš”.  ê·¸ë˜ì„œ ì´ë²ˆì— ê·¸ë¦¬ë“¤íŒ¬ ìƒˆë¡œ ì¥ë§Œí•´ë´¤ìŠµë‹ˆë‹¤^^  ê¸°ì¡´ì— 33cm ì“°ë‹¤ê°€ ë°”ê¿€ ë•Œ ë˜ì–´ì„œ êµ¬ì…í–ˆì–´ìš”. 36, 40ì¤‘ì—ì„œ ê³ ë¯¼í•˜ë‹¤ 40 ìƒ€ëŠ”ë° ìƒê°ë³´ë‹¤ë§ì´ í¬ë„¤ìš”. ì•„ë¬´ë˜ë„ ìº í•‘ì— ê°€ì§€ê³  ë‹¤ë‹ˆê¸°ì—” ë¶€í”¼ê°€ ë„ˆë¬´ í¬ë„¤ìš”. ê·¸ë˜ë„ ê°€ê²©ë„ ì¢‹ê³  ê¹Šì´ê°ë„ ìˆì–´ ì¢‹ì•„ìš”. ìº í•‘ì€ ì•„ì§ ëª» ê°€ì„œ ì§‘ì—ì„œ ë¼ë©´ë„ ë¨¹ê³  ë³¶ìŒë°¥ë„ ë¨¹ê³  ê³ ê¸°ë„ ë¨¹ì—ˆëŠ”ë° ë§›ìˆê²Œ ì˜ë¼ìš”ã… êµ¬ì„±í’ˆì— ê°€ë°©ê¹Œì§€ ìˆì–´ì„œ ì¢‹ì•„ìš”.  ìƒìê°€ ë‹¤ ì°¢ì–´ì§€ê³  ë°°ì†¡ì€ ì™„ì „ ì—‰ë§ì´ì—ˆì–´ìš”.ã… ã…  ë‹¤í–‰íˆ ìƒí’ˆì€ ì´ìƒ ì—†ì—ˆì§€ë§Œ, ì²˜ìŒ ì°¢ì–´ì§„ ìƒìë¥¼ ë´¤ì„ë• ë§ì´ ë†€ë¬ë„¤ìš” ã…‹ã…‹  ì—¬ê¸°ê¹Œì§€ ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê·¸ë¦¬ë“¤ êµ¬ë§¤ í›„ê¸°ë¥¼ ë‚¨ê²¨ë´¤ìŠµë‹ˆë‹¤~',\n",
       " 'id': 1035734,\n",
       " 'label': 0.0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì£¼ë³€ì‚¬ëŒ ì¶”ì²œí…œìœ¼ë¡œ ì¸ê¸°ê°€ ë§ì€ ìƒí’ˆì„ ì§€ê¸ˆë¶€í„° ì†Œê°œí•©ë‹ˆë‹¤.  ëŠ˜ ì‚¬ìš©í•˜ê³  ìˆëŠ” ì œí’ˆì…ë‹ˆë‹¤. ë³€ê¸°ì— ìˆëŠ” ë¬¼ë•Œë¥¼ ê¹¨ë—í•˜ê²Œ ì œê±°í•´ì£¼ë‹ˆ ì •ë§ ì¢‹ì€ ê²ƒ ê°™ì•„ìš”. ë˜í•œ ì˜¤ì—¼ë°©ì§€ ê¸°ëŠ¥ë„ ìˆì–´ì„œ ì°¸ ì¢‹ìŠµë‹ˆë‹¤. ë°©í–¥ì œ íš¨ê³¼ë„ ìˆì–´ì„œ ì‚¬ìš©í•  ë•Œ í–¥ì´ ì€ì€í•˜ê²Œ ë‚˜ë‹ˆ ë” ì¢‹ì€ ê²ƒ ê°™ì•„ìš”. ë‹¤ë¥¸ í–¥ë³´ë‹¤ ë ˆëª¬í–¥ì´ ì€ì€í•˜ê²Œ ì˜¤ë˜ ì§€ì†ë˜ëŠ” ì ì´ ì°¸ ì¢‹ì€ ê²ƒ ê°™ë„¤ìš”. ê°€ê²©ì´ ê·¸ë ‡ê²Œ ì €ë ´í•œ í¸ì€ ì•„ë‹ˆì§€ë§Œ ë‹¤ ì“°ê³  ë‚˜ë©´ ì¬êµ¬ë§¤í•  ì˜ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤. ë„ˆë¬´ ì¢‹ì•„ìš”.  ë‚´ ì‚¶ì„ ë”ìš± ì—…ê·¸ë ˆì´ë“œí•´ì£¼ëŠ” ì‡í…œì´ ë ê²ƒ ê°™ìŠµë‹ˆë‹¤.</td>\n",
       "      <td>1035953</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ì•„ê¸°ë•Œë¶€í„° ì§€ê¸ˆ ì–´ë¥¸ì´ ëœ ì´ ìˆœê°„ê¹Œì§€ ê¾¸ì¤€íˆ ì œ ê³ì„ ì§€ì¼œì¤€ ì´ê°€ ìˆëŠ”ë°ìš”. ê¶ê¸ˆí•˜ì‹œë‹¤ë©´ ì§€ê¸ˆë¶€í„° í›„ê¸°ì†ìœ¼ë¡œ ë¹ ì ¸ë³´ê² ìŠµë‹ˆë‹¤.  í–¥ì´ ì€ì€í•˜ê²Œ í¼ì ¸ì„œ ë§ˆìŒê¹Œì§€ í¸ì•ˆí•˜ê²Œ ë§Œë“¤ì–´ì£¼ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì„¸ì•ˆë¿ë§Œ ì•„ë‹ˆë¼ ë°”ë””ìš©ìœ¼ë¡œë„ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì„¸ì•ˆì„ í–ˆì„ ë•Œ ë‹¹ê¸°ì§€ ì•Šê³  ì´‰ì´‰í•´ì„œ ì°¸ ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”. ìœ í†µê¸°í•œë„ 2ë…„ì´ë‚˜ ë‚¨ì•„ì„œ ì—„ì²­ ë„‰ë„‰í•©ë‹ˆë‹¤. ì˜ êµ¬ë§¤í•œ ê²ƒ ê°™ë„¤ìš”.  ì˜¤ëŠ˜ì€ ì—¬ê¸°ê¹Œì§€ ì…ë‹ˆë‹¤ ë‹¤ì†Œ ë¶€ì¡±í•˜ì§€ë§Œ ì´í•´í•´ ì£¼ì„¸ìš” ë‹¤ìŒì—ëŠ” ë‹¤ë¥¸ ë‚´ìš©ìœ¼ë¡œ ë‹¤ì‹œì˜¬ê²Œìš”!</td>\n",
       "      <td>1037003</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40ëŒ€ê°€ ë˜ì—ˆìŒì—ë„ ã…‡ã…‡ëŠ” ì˜ì›í•œ ì‚¬ë‘ì´ë„¤ìš” ì˜¤ëŠ˜ì€ ã…‡ã…‡ íœ´ì§€ë¥¼ ì†Œê°œí•´ ë³¼ê¹Œ í•©ë‹ˆë‹¤  ã…‡ã…‡ë¥¼ ë„ˆë¬´ ì¢‹ì•„í•´ì„œ í•œ ë²ˆ ì£¼ë¬¸í•´ë´¤ë‹¤ê°€ ë¨¼ì§€ë„ ì•ˆ ë‚˜ê³  ë¶€ë“œëŸ¬ì›Œì„œ ëª‡ ë…„ì§¸ ì‚¬ìš©ì¤‘ì…ë‹ˆë‹¤ì¼ë‹¨ ë””ìì¸ì´ ã…‡ã…‡ë¼ ì‚¬ìš©í•˜ë©´ì„œë„ í–‰ë³µí•˜ë„¤ìš” ë¡¤ë„ ë‹¨ë‹¨íˆ ê°ê²¨ ìˆê³  ë¨¼ì§€ë„ ê±°ì˜ ì•ˆë‚˜ì„œ ë„ˆë¬´ ì¢‹ìŠµë‹ˆë‹¤ ì¬ì§ˆë„ ê³ ê¸‰ì§€ê³  ë¶€ë“œëŸ¬ì›€ì€ ë˜ ë§í•  ìˆ˜ ì—†ì´ ë¶€ë“œëŸ¬ì›Œìš” ë‘ê»˜ë„ ì‚¬ìš©í•˜ê¸° ì ë‹¹í•©ë‹ˆë‹¤ ë„ˆë¬´ ë‘êº¼ì›Œë„ ë¶ˆí¸í•  ë•Œê°€ ìˆê±°ë“ ìš” ê°€ê²©ë„ ã…‡ã…‡ì§€ë§Œ ì €ë ´í•˜ê³  í–‰ì‚¬í•˜ë©´ ë” ì¢‹ì€ ê°€ê²©ì— êµ¬ë§¤í•  ìˆ˜ ìˆì–´ìš” ê°•ì¶”í•©ë‹ˆë‹¤  í¬ìŠ¤íŒ…ì„ ë³´ì‹œê³  ë” ì¢‹ì€ ì„ íƒì„ í• ìˆ˜ ìˆëŠ” ì˜ë¯¸ìˆëŠ” ê¸€ì´ì˜€ìœ¼ë©´ ì¢‹ê² ë„¤ìš”</td>\n",
       "      <td>1033869</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ì˜¤ë˜ëœ ê°œì¸ì£¼íƒì´ë¼ ì—¬ê¸°ì €ê¸° ìƒê¸°ëŠ” ê³°íŒ¡ì´ ë•Œë¬¸ì— ê³ ë¯¼ì´ ë§ì•˜ëŠ”ë° ì§€ì¸ì´ ì†Œê°œí•´ ì¤€ ê³°íŒ¡ì´ ì œê±°ì œê°€ ìˆì–´ ì†Œê°œí•´ë“œë¦¬ë ¤êµ¬ìš”.  ë²½ì§€ì— ê²°ë¡œë•œì— ê³°íŒ¡ì´ê°€ ìƒê²¨ì„œ ëœ¯ì–´ë‚´ë³´ë‹ˆ ì•ˆìª½ì— ê³°íŒ¡ì´ê°€ ê°€ë“í–ˆì–´ìš”. ë„ˆë¬´ ì—¬ëŸ¬êµ°ë° ê³°íŒ¡ì´ê°€ ë§ì•„ì„œ ë¿Œë¦¬ê³  15ë¶„ ì •ë„ ë‘”ê±° ê°™ì•„ìš”. ê·¸ë¦¬ê³  ì –ì€ ê±¸ë ˆë¡œ ë‹¦ì•„ëƒˆëŠ”ë° ì–´ë¨¸ë‚˜~!!! ê¹œìª½ê°™ì´... ì§„ì§œ ê¹¨ë—í•˜ê²Œ ë‹¦ì˜€ì–´ìš”. ëŒ€ì‹  ê·¸ë§Œí¼ ëƒ„ìƒˆë„ ë…í•œê±° ê°™ì•„ì„œ í™˜ê¸°ë¥¼ ì˜¤ë«ë™ì•ˆ í–ˆì§€ìš”. ì‚¬ìš©í•  ë•Œ ë§ˆìŠ¤í¬ë„ ì˜ë¼ê³  í–ˆêµ¬ìš”...ê·¼ë° ë¿Œë ¤ë§Œ ë‘ê³  ë¬¼ë¡œ ì”»ì–´ë‚¸ë‹¤ê³  í•´ì„œ ê¹¨ë—ì´ ë‹¦ì´ì§€ëŠ” ì•Šì•„ìš”. ì†” ë“±ìœ¼ë¡œ ë¬¸ì§ˆëŸ¬ì£¼ë©´ íš¨ê³¼ê°€ ë” ì¢‹ìŠµë‹ˆë‹¤.  ê³°íŒ¡ì´ ì œê±° ë¿ë§Œì•„ë‹ˆë¼ íš¨ê³¼ë„ ì˜¤ë˜ ì§€ì†ë˜ëŠ” ì œí’ˆì´ë¼ ì¢‹ì€ ì •ë³´ ë“œë¦¬ê³  ì‹¶ì—ˆì–´ìš”. ë‹¤ìŒì— ë˜ ë‹¤ë¥¸ ì œí’ˆ í¬ìŠ¤íŒ… ì˜¬ë¦´ê»˜ìš”.</td>\n",
       "      <td>1038029</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ì—¬ëŸ¬ë¶„ì€ ì§€ê¸ˆ ì–´ë–¤ ìƒê°ì„ í•˜ê³  ìˆìŠµë‹ˆê¹Œ? í˜¹ì‹œ ã…‡ã…‡ã…‡ ì¹˜ì•½ì˜ í›„ê¸°ë¥¼ ê¸°ë‹¤ë¦¬ê³  ê³„ì‹œì§„ ì•Šì•˜ë‚˜ìš”? ì§€ê¸ˆ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.  ì‡ëª¸ë•Œë¬¸ì— ì¹˜ê³¼ë¥¼ ë‹¤ë‹ˆë‹¤ê°€ ì¶”ì²œì„ ë°›ì•„ì„œ êµ¬ë§¤ë¥¼ í–ˆìŠµë‹ˆë‹¤. ì´ì‹œë¦¼ì´ íš¨ê³¼ê°€ ìˆì„ê¹Œ ìƒê°ì„ í–ˆëŠ”ë° í™•ì‹¤íˆ ì¢‹ì•„ì¡Œë„¤ìš”. ì´ë¥¼ ë‹¦ê³  ë‚˜ë©´ ì…ì•ˆì´ ìƒì¾Œí•´ì„œ ì¢‹ìŠµë‹ˆë‹¤. ê°œìš´í•¨ì´ ì˜¤ë˜ìœ ì§€ë˜ëŠ” ì ë„ ì¢‹ì€ ê²ƒ ê°™ê³ ìš”. ê°€ê²©ì´ ë¹„ì‹¼í¸ì¸ë° ì˜¨ë¼ì¸ìœ¼ë¡œ êµ¬ë§¤í•´ë„ ë¹„ì‹¼ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë§Œì¡±ìŠ¤ëŸ¬ìš´ ì¹˜ì•½ì„ ë§Œë‚œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.  ì €ëŠ” ì´ë§Œ ë¬¼ëŸ¬ê°ˆê»˜ìš”! ë¶€ì¡±í•œ ê¸€ì´ì§€ë§Œ ëê¹Œì§€ ì½ì–´ì£¼ì…”ì„œ ë„ˆë¬´ ê³ ë§ˆì›Œìš”~ ë‹¤ìŒì—ë„ ë§ì´ ì¤€ë¹„ í• í…Œë‹ˆ ë˜ ì˜¤ì„¸ìš”</td>\n",
       "      <td>1036956</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ë§ë³µë§Œ ì§€ë‚˜ë©´ ì´ì œ ì½”ìŠ¤ëª¨ìŠ¤ í”¼ê³  ì•„ì¹¨ì €ë…ìœ¼ë¡œ ì„œëŠ˜í•œ ë°”ëŒì´ ë¶ˆ ê²ƒì…ë‹ˆë‹¤. ê·¸ ìƒê°ìœ¼ë¡œ ê¸°ìš´ì„ ì°¨ë ¤ ì˜¤ëŠ˜ì„ ë³´ë‚´ì•¼ í•˜ê² ìŠµë‹ˆë‹¤.  ì „ì— ì‚¬ìš©í•˜ë˜ ë¨¼ì§€í„¸ì´ê°œê°€ ë„ˆë¬´ ì˜¤ë˜ë˜ê³  ì˜ ì•ˆ ë‹¦ì—¬ì„œ ìƒˆìƒí’ˆìœ¼ë¡œ êµ¬ì…ì„ í–ˆìŠµë‹ˆë‹¤ í™”ë©´ ê·¸ëŒ€ë¡œ ë””ìì¸ì´ ì˜ˆë»ì„œ ë§˜ì— ë“­ë‹ˆë‹¤ ì°¨ëŸ‰ì˜ ë¨¼ì§€ë„ ì‹œì›í•˜ê²Œ ì˜ í„¸ë ¤ì„œ ì•„ì£¼ ì¢‹ìŠµë‹ˆë‹¤ ê·¹ì„¸ì‚¬ë¼ì„œ ì°¨ëŸ‰ì— í ì§‘ì´ ì•ˆë‚˜ì„œ ë§ˆìŒì— ë“¤ì–´ìš” ì†ì¡ì´ ê¸¸ì´ë„ ëŠ˜ë ¤ì ¸ì„œ ì‚¬ìš©í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤ ê°€ê²©ë„ ì €ë ´í•´ì„œ ë”± ì¢‹ë„¤ìš”  ì˜¤ëŠ˜ ë‚´ìš©ì´ ì¢‹ì€ ì •ë³´ê°€ ë˜ì˜€ë‚˜ìš”? ê·¸ë¬ë‹¤ë©´ ì¢‹ì•„ìš”ì™€ ëŒ“ê¸€ ë¶€íƒë“œë ¤ìš” ë‹¤ìŒì—ë„ ë” ì¢‹ì€ ë‚´ìš©ìœ¼ë¡œ ì°¾ì•„ëµê»˜ìš”</td>\n",
       "      <td>1034626</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ë©°ì¹ ì „ ê¸¸ì„ ê±·ë‹¤ ì•ì— ê±¸ì–´ê°€ëŠ” ì‚¬ëŒì—ê²Œì„œ ë‚˜ëŠ” ì€ì€í•œ í–¥ì´ ê¼­ ì´ í–¥ ê°™ì•˜ì–´ìš”   í–¥ë„ ì€ì€í•˜ê²Œ ì˜¤ë˜ ë‚¨ì•„ì„œ ë§¤ì¼ ì“°ê¸° ì¢‹ì•„ìš” ê°€ë” í–¥ì´ ê°•í•œ íƒ€ì‚¬ ì œí’ˆ ì„¬ìœ ìœ ì—°ì œëŠ” ë¨¸ë¦¬ ê°€ ì•„í”ˆë° ê·¸ë ‡ì§€ ì•Šì•„ ì¢‹ë„¤ìš” ê³ ê¸‰ìŠ¤ëŸ¬ìš´ ì€ì€í•œí–¥ì´ë¼ ë§˜ì— ë“¤ê³  ì‹œíŠ¸í˜•ì€ ì²˜ìŒ ì‚¬ìš©í•´ë´¤ ëŠ”ë° ì •ë§ í¸í•˜ê³  í•œì¥ì”© ë‚±ê°œí¬ì¥ì´ë¼ ë” ì‚¬ìš©í•˜ê¸° í¸í•˜ê³  ì¢‹ì•„ìš”. ë°˜ì‹ ë°˜ì˜í•˜ë©° ìƒ€ëŠ”ë° ë¹„ëˆ„í–¥ì´ ì •ë§ ì¢‹ë„¤ìš” ë‹¨ì ì€ ì¢€ ë¹„ì‹¸ë„¤ìš” ì—¬ê¸°ì—ì„œ ì¢‹ì€ ê°€ê²©ì— ì‚¬ê¸´í–ˆì–´ìš” ë¹„ì‹¼ë§Œí¼ íš¨ê³¼ê°€ ì¢‹ìœ¼ë‹ˆ ë§Œì¡±í•©ë‹ˆë‹¤  ëŠ˜ ì‚¬ìš©í•˜ëŠ” ì„¬ìœ ìœ ì—°ì œì§€ë§Œ ë§ˆìŒì— ë“œëŠ” ì œí’ˆ ì—†ìœ¼ì‹œë‹¤ë©´ ì´ ì œí’ˆ í•œë²ˆ ì‚¬ìš©í•´ë³´ì‹œë¼ê³  í•˜ ê³  ì‹¶ë„¤ìš”</td>\n",
       "      <td>1026944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ì•ˆë…•í•˜ì„¸ìš” ì´ì›ƒë‹˜ë“¤~~OOO ë°”ë¥´ëŠ” ê³°íŒ¡ì´ì‹¹ ì‚¬ìš©í›„ê¸° ì§€ê¸ˆ ì‹œì‘í•©ë‹ˆë‹¤.  ìš•ì‹¤ ë²½ ëª¨í‰ì´ì— ê³°íŒ¡ì´ê°€ ìƒê²¨ ì­‰ ì§œë‘ê³  í•œì‹œê°„ ë°©ì¹˜í•˜ë‹ˆ ë…¸ë—ê²Œ ê³°íŒ¡ì´ ë¶€ë¶„ì´ ë¶„í•´ë˜ë”ë‹ˆ ì ¤ë„ ë…¸ë€ ìƒ‰ìœ¼ë¡œ ë°”ë€Œë”ë¼êµ¬ìš”. ë…í•œ ëƒ„ìƒˆê°€ ë‚©ë‹ˆë‹¤. í˜¸í¡ê¸°ì—” ì¢‹ì§€ ì•Šìœ¼ë‹ˆ ë§ˆìŠ¤í¬ ì“°ê³  í•˜ëŠ” ê²Œ ì¢‹ë‹¤ê³  ë´ìš”. ë…¸ë—ê²Œ ë³€í•œ ì ¤ì„ ë‹¦ê³  í™˜ê¸°ë¥¼ ì‹œì¼°ì–´ìš”. í™•ì—°íˆ ì—†ì–´ì§„ ê³°íŒ¡ì´ë¥¼ ë³´ë‹ˆ ì†ì´ ë‹¤ ì‹œì›í•©ë‹ˆë‹¤. ë¬½ì§€ ì•Šì•„ ì£¼ë¥´ë¥µ í˜ëŸ¬ë‚´ë¦¬ì§€ ì•Šì•„ì„œ ì´ë ‡ê²Œ ì• ë§¤í•œ ìŠ¤íŒŸì—ë„ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹ˆ ì¢‹ì•„ìš”. ì •ë§ ê³ ë‚œë„ì˜ ìŠ¤í‚¬ì„ í•„ìš”ë¡œ í•˜ì§€ ì•Šì•„ ê°•ì¶”í•©ë‹ˆë‹¤.ë‹¨, ì˜¤ë˜ë™ì•ˆ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ì¢€ êµ³ì–´ìš”.  ì„¤ëª… ë!! ì¢€ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”? ë„ì›€ì´ ë˜ì…¨ë‹¤ë©´ ì „ ë„ˆë¬´ í–‰ë³µí•´ìš”~~</td>\n",
       "      <td>1030262</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ë§ì€ ë¶„ë“¤ì—ê²Œ ì¸ê¸°ê°€ ë§ì€ ã…‡ã…‡ã…‡ í™”ì¥ì§€ í›„ê¸°ë¥¼ ì§€ê¸ˆë¶€í„° ì‘ì„±í•˜ë ¤ê³  í•©ë‹ˆë‹¤  1+1 í–‰ì‚¬ë¼ ë„ˆë¬´ ì¢‹ë„¤ìš” ã…‡ã…‡ã…‡ ë¹„ì‹¼ ì œí’ˆì´ë¼ ëŠ˜ ë§ì„¤ì´ëŠ”ë° ì´ë²ˆì— í•˜ë‚˜ ê°€ê²©ìœ¼ë¡œ ë³´ë‹ˆ ë„ˆë¬´ ì €ë ´í•œê±° ê°™ì•„ ë°”ë¡œ êµ¬ë§¤í–ˆìŠµë‹ˆë‹¤ 3ê²¹ì´ë¼ ë‘íˆ¼í•˜ê³  íƒ€ë¸Œëœë“œ 3ê²¹ë³´ë‹¤ ë” ë‘íˆ¼í•©ë‹ˆë‹¤ ê¸¸ì´ë„ 30më‚˜ ë˜ë‹ˆ ì¢‹ê³  í–¥ë„ ì€ì€í•´ì„œ ë”ìš± ë§ˆìŒì— ë“­ë‹ˆë‹¤ ã…‡ã…‡ã…‡ íœ´ì§€ëŠ” ë¨¼ì§€ë„ ëœí•˜ë„¤ìš” ì•„ì˜ˆ ì—†ëŠ”ê±´ ì•„ë‹Œë° ì‹ ê²½ ì“°ì¼ ì •ë„ëŠ” ì•„ë‹™ë‹ˆë‹¤ 30ë¡¤ 2íŒ©í•´ì„œ 60ë¡¤ì´ë‚˜ ë˜ë‹ˆ ì˜¤ë«ë™ì•ˆ ì˜ ì‚¬ìš©í•˜ê² ì–´ìš” ë§Œì¡±í•©ë‹ˆë‹¤  ì œ í›„ê¸°ë¡œ ì¸í•´ êµ¬ë§¤ì‹œ ë§ì€ ë„ì›€ì´ ë˜ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤</td>\n",
       "      <td>1033795</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ì˜¤ëŠ˜ë„ ë°©ë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì˜¤ëŠ˜ì˜ í›„ê¸°ëŠ” ë°€íìš©ê¸°ì…ë‹ˆë‹¤.  ìš°ë¦¬ ë°€íìš©ê¸°ë„ ì´ì™• ì˜ˆìœ ê²Œ ì¢‹ì–ì•„ìš”? ì €ëŠ” ë””ìì¸ì´ ì´ì˜ë©´ ì €ë„ ëª¨ë¥´ê²Œ ì‚¬ëŠ” ê²½í–¥ì´ ìˆ ëŠ”ë°ìš”. ì´ ì œí’ˆ ëšœê»‘ ìƒ‰ì´ íŒŒìŠ¤í…” í†¤ìœ¼ë¡œ ë„ˆë¬´ ì˜ˆë»ì„œ ìƒ‰ì€ ì •ë§ ë§ˆìŒì— ë“¤ì—ˆëŠ”ë°ìš”.  ëšœê»‘ ì†Œì¬ê°€ ì¢€ ì €ë ´í•œ í”Œë¼ìŠ¤í‹± ëŠë‚Œì´ì–´ì„œ ì˜¤ë˜ ì“¸ ìˆ˜ ìˆì„ì§€ëŠ” ì˜ë¬¸ì´ì—ˆì–´ìš”. ê·¸ë¦¬ê³  ì „ìë ˆì¸ì§€ì— ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  í•´ì„œ ëŒë ¸ëŠ”ë° ì°Œê·¸ëŸ¬ì¡Œì–´ìš”. ì „ìë ˆì¸ì§€ì— ì˜¤ë˜ ëŒë¦´ ìˆ˜ ìˆëŠ” ì†Œì¬ëŠ” ì•„ë‹Œ ê²ƒ ê°™ì•„ìš”.  ì˜¬ í•œí•´ ì˜ ë§ˆë¬´ë¦¬í•˜ì‹œê³  í•­ìƒ ê±´ê°•í•˜ê³  í–‰ë³µí•˜ì„¸ìš”.</td>\n",
       "      <td>1032748</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLUE_TASKS = [\"cola\", \"mnli\", \"mnli-mm\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"sst2\", \"stsb\", \"wnli\"]\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.8k/28.8k [00:00<00:00, 49.9MB/s]\n",
      "Downloading metadata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.7k/28.7k [00:00<00:00, 49.7MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27.9k/27.9k [00:00<00:00, 52.7MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 377k/377k [00:00<00:00, 11.5MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8551/8551 [00:00<00:00, 80758.31 examples/s]\n",
      "Generating validation split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 84648.98 examples/s]\n",
      "Generating test split: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1063/1063 [00:00<00:00, 79723.65 examples/s]\n",
      "/tmp/ipykernel_42102/1389288479.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('glue', actual_task)\n",
      "Downloading builder script: 5.76kB [00:00, 14.2MB/s]                   \n"
     ]
    }
   ],
   "source": [
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "metric = load_metric('glue', actual_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': \"Our friends won't buy this analysis, let alone the next one we propose.\",\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"glue\", features: {'predictions': Value(dtype='int64', id=None), 'references': Value(dtype='int64', id=None)}, usage: \"\"\"\n",
       "Compute GLUE evaluation metric associated to each GLUE dataset.\n",
       "Args:\n",
       "    predictions: list of predictions to score.\n",
       "        Each translation should be tokenized into a list of tokens.\n",
       "    references: list of lists of references for each translation.\n",
       "        Each reference should be tokenized into a list of tokens.\n",
       "Returns: depending on the GLUE subset, one or several of:\n",
       "    \"accuracy\": Accuracy\n",
       "    \"f1\": F1 score\n",
       "    \"pearson\": Pearson Correlation\n",
       "    \"spearmanr\": Spearman Correlation\n",
       "    \"matthews_correlation\": Matthew Correlation\n",
       "Examples:\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'sst2')  # 'sst2' or any of [\"mnli\", \"mnli_mismatched\", \"mnli_matched\", \"qnli\", \"rte\", \"wnli\", \"hans\"]\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'mrpc')  # 'mrpc' or 'qqp'\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'accuracy': 1.0, 'f1': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'stsb')\n",
       "    >>> references = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> predictions = [0., 1., 2., 3., 4., 5.]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print({\"pearson\": round(results[\"pearson\"], 2), \"spearmanr\": round(results[\"spearmanr\"], 2)})\n",
       "    {'pearson': 1.0, 'spearmanr': 1.0}\n",
       "\n",
       "    >>> glue_metric = datasets.load_metric('glue', 'cola')\n",
       "    >>> references = [0, 1]\n",
       "    >>> predictions = [0, 1]\n",
       "    >>> results = glue_metric.compute(predictions=predictions, references=references)\n",
       "    >>> print(results)\n",
       "    {'matthews_correlation': 1.0}\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)okenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.0/28.0 [00:00<00:00, 167kB/s]\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 483/483 [00:00<00:00, 3.66MB/s]\n",
      "Downloading (â€¦)solve/main/vocab.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 232k/232k [00:00<00:00, 12.2MB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466k/466k [00:00<00:00, 2.70MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2028, 6251, 999, 102, 1998, 2023, 6251, 3632, 2007, 2009, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this one sentence!\", \"And this sentence goes with it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Our friends won't buy this analysis, let alone the next one we propose.\n"
     ]
    }
   ],
   "source": [
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True)\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 2256, 2814, 2180, 1005, 1056, 4965, 2023, 4106, 1010, 2292, 2894, 1996, 2279, 2028, 2057, 16599, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 1998, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 2028, 2062, 18404, 2236, 3989, 2030, 1045, 1005, 1049, 3228, 2039, 1012, 102], [101, 1996, 2062, 2057, 2817, 16025, 1010, 1996, 13675, 16103, 2121, 2027, 2131, 1012, 102], [101, 2154, 2011, 2154, 1996, 8866, 2024, 2893, 14163, 8024, 3771, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8551/8551 [00:00<00:00, 96076.09 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1043/1043 [00:00<00:00, 86098.39 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1063/1063 [00:00<00:00, 71780.04 examples/s]\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:23<00:00, 11.4MB/s] \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task==\"stsb\" else 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb ì…€ 21\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m validation_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mvalidation_mismatched\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmnli-mm\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation_matched\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m task \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmnli\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     args,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mencoded_dataset[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39;49mencoded_dataset[validation_key],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     compute_metrics\u001b[39m=\u001b[39;49mcompute_metrics\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/nuri/dev/sources/git/iterative_ai_test/test.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/transformers/trainer.py:559\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpush_to_hub:\n\u001b[0;32m--> 559\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_hf_repo()\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mshould_save:\n\u001b[1;32m    561\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moutput_dir, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/transformers/trainer.py:3435\u001b[0m, in \u001b[0;36mTrainer.init_hf_repo\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3433\u001b[0m     repo_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mhub_model_id\n\u001b[0;32m-> 3435\u001b[0m repo_url \u001b[39m=\u001b[39m create_repo(repo_name, token\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_token, private\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs\u001b[39m.\u001b[39;49mhub_private_repo, exist_ok\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   3436\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhub_model_id \u001b[39m=\u001b[39m repo_url\u001b[39m.\u001b[39mrepo_id\n\u001b[1;32m   3437\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpush_in_progress \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:2542\u001b[0m, in \u001b[0;36mHfApi.create_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   2538\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_lfsmultipartthresh\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   2539\u001b[0m     \u001b[39m# Testing purposes only.\u001b[39;00m\n\u001b[1;32m   2540\u001b[0m     \u001b[39m# See https://github.com/huggingface/huggingface_hub/pull/733/files#r820604472\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     json[\u001b[39m\"\u001b[39m\u001b[39mlfsmultipartthresh\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lfsmultipartthresh  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2542\u001b[0m headers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_build_hf_headers(token\u001b[39m=\u001b[39;49mtoken, is_write_action\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   2543\u001b[0m r \u001b[39m=\u001b[39m get_session()\u001b[39m.\u001b[39mpost(path, headers\u001b[39m=\u001b[39mheaders, json\u001b[39m=\u001b[39mjson)\n\u001b[1;32m   2545\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py:5719\u001b[0m, in \u001b[0;36mHfApi._build_hf_headers\u001b[0;34m(self, token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   5716\u001b[0m \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5717\u001b[0m     \u001b[39m# Cannot do `token = token or self.token` as token can be `False`.\u001b[39;00m\n\u001b[1;32m   5718\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoken\n\u001b[0;32m-> 5719\u001b[0m \u001b[39mreturn\u001b[39;00m build_hf_headers(\n\u001b[1;32m   5720\u001b[0m     token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   5721\u001b[0m     is_write_action\u001b[39m=\u001b[39;49mis_write_action,\n\u001b[1;32m   5722\u001b[0m     library_name\u001b[39m=\u001b[39;49mlibrary_name \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary_name,\n\u001b[1;32m   5723\u001b[0m     library_version\u001b[39m=\u001b[39;49mlibrary_version \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlibrary_version,\n\u001b[1;32m   5724\u001b[0m     user_agent\u001b[39m=\u001b[39;49muser_agent \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m   5725\u001b[0m )\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:122\u001b[0m, in \u001b[0;36mbuild_hf_headers\u001b[0;34m(token, is_write_action, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[39m# Get auth token to send\u001b[39;00m\n\u001b[1;32m    121\u001b[0m token_to_send \u001b[39m=\u001b[39m get_token_to_send(token)\n\u001b[0;32m--> 122\u001b[0m _validate_token_to_send(token_to_send, is_write_action\u001b[39m=\u001b[39;49mis_write_action)\n\u001b[1;32m    124\u001b[0m \u001b[39m# Combine headers\u001b[39;00m\n\u001b[1;32m    125\u001b[0m headers \u001b[39m=\u001b[39m {\n\u001b[1;32m    126\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39muser-agent\u001b[39m\u001b[39m\"\u001b[39m: _http_user_agent(\n\u001b[1;32m    127\u001b[0m         library_name\u001b[39m=\u001b[39mlibrary_name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     )\n\u001b[1;32m    131\u001b[0m }\n",
      "File \u001b[0;32m~/dev/sources/git/iterative_ai_test/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py:172\u001b[0m, in \u001b[0;36m_validate_token_to_send\u001b[0;34m(token, is_write_action)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39mif\u001b[39;00m is_write_action:\n\u001b[1;32m    171\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    173\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mToken is required (write-access action) but no token found. You need\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m to provide a token or be logged in to Hugging Face with\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `huggingface-cli login` or `huggingface_hub.login`. See\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m https://huggingface.co/settings/tokens.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    177\u001b[0m         )\n\u001b[1;32m    178\u001b[0m     \u001b[39mif\u001b[39;00m token\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mapi_org\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    179\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    180\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou must use your personal account token for write-access methods. To\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    181\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m generate a write-access token, go to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m https://huggingface.co/settings/tokens\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    183\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Token is required (write-access action) but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens."
     ]
    }
   ],
   "source": [
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
